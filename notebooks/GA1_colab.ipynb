# ==== Colab installs (keep as-is) ====
!pip install adjustText
!pip install folium
!pip install pillow

# ==== RIG SCHEDULING OPTIMIZATION WITH GENETIC ALGORITHM (GA-1) ====
# Objective: Optimize rig scheduling using GA considering distance/time and BOPD
# Notes:
# - This notebook mirrors the original GA-1 code with minimal/no logic changes.
# - Only labels, titles, legends, and comments are translated to English.
# - If your data is on Google Drive, mount Drive and adjust the file path.

# ==== LIBRARIES ====
import pandas as pd
import numpy as np
import random
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.cm import get_cmap
from matplotlib.collections import LineCollection
from adjustText import adjust_text

# Extras
from datetime import timedelta, datetime      # If you simulate travel time and execution dates
from sklearn.preprocessing import MinMaxScaler # If you plan to normalize before GA
from itertools import permutations             # Useful for brute-force (non-GA baseline)

# ==== LOAD DATA ====
# If using Google Drive, set: DATASET_PATH = "/content/drive/MyDrive/THESIS/..."
# df = pd.read_excel(DATASET_PATH)

df = pd.read_excel("Variant-4_RIG-24_WOWS_Mina_2022-2024.xlsx")
df.head()

# ==== QUICK DATA CHECKS (English labels only) ====
print("Columns:", df.columns)
print("Shape:", df.shape)
print("Missing values per column:\n", df.isnull().sum())
print("Duplicate rows:", df.duplicated().sum())

# ==== DATE COLUMNS ====
# Convert date columns to datetime where relevant
df['REQ_DT'] = pd.to_datetime(df['REQ_DT'])
df['START_DATETIME_JOB'] = pd.to_datetime(df['START_DATETIME_JOB'])
df['END_DATETIME_JOB'] = pd.to_datetime(df['END_DATETIME_JOB'])
df['Month'] = df['START_DATETIME_JOB'].dt.to_period('M')  # monthly period bucket
df.info()

# Show missing counts again (optional)
df.isnull().sum()

# ==== BASIC EDA ====
df.describe()

# Print the first and last start dates (raw START_DATE_JOB; converted later before scatter)
first_start = df['START_DATE_JOB'].min()
last_start = df['START_DATE_JOB'].max()
print(f"First start date: {first_start}")
print(f"Last start date: {last_start}")

# Sanity assert: ensure no job has end date earlier than start date
# (comment out if your raw data intentionally violates this)
assert all(df['END_DATETIME_JOB'] >= df['START_DATETIME_JOB'])

# Show the route order horizontally (based on original actual sequence)
print(" → ".join(df['WELL_ALIAS'].tolist()))

# ==== MONTHLY BASELINE: per year ====
# Remove helper columns if they already exist
for col in ['Month_Label', 'Month_Sort', 'Year']:
    if col in df.columns:
        df.drop(columns=[col], inplace=True)

# Re-create helper columns
df['Month_Label'] = df['START_DATETIME_JOB'].dt.strftime('%B %Y')
df['Month_Sort']  = df['START_DATETIME_JOB'].dt.to_period('M').astype(str)
df['Year']        = df['START_DATETIME_JOB'].dt.year

# Group by month (per year)
month_counts = df.groupby(['Year', 'Month_Sort', 'Month_Label']).size().reset_index(name='Jumlah')
month_counts = month_counts.sort_values(by=['Year', 'Month_Sort'])

# Plot per year with yearly average line
years = month_counts['Year'].unique()
for year in years:
    data_year = month_counts[month_counts['Year'] == year]
    avg_jumlah = data_year['Jumlah'].mean()

    plt.figure(figsize=(10,5))
    plt.bar(data_year['Month_Label'], data_year['Jumlah'], color='green')
    plt.axhline(y=avg_jumlah, color='red', linestyle='--', linewidth=2,
                label=f'Avg {year}: {avg_jumlah:.1f} jobs/month')

    plt.title(f'BASELINE — Actual Execution Rate per Month ({year})')
    plt.xlabel('Month')
    plt.ylabel('Jobs Executed')
    plt.xticks(rotation=45)
    plt.legend()
    plt.tight_layout()
    plt.show()

# ==== MONTHLY BASELINE: all years combined ====
# Remove helper columns if they already exist
for col in ['Month_Label', 'Month_Sort', 'Year']:
    if col in df.columns:
        df.drop(columns=[col], inplace=True)

# Re-create helper columns (short month label)
df['Month_Label'] = df['START_DATETIME_JOB'].dt.strftime('%b %Y')  # shorter month name
df['Month_Sort']  = df['START_DATETIME_JOB'].dt.to_period('M').astype(str)
df['Year']        = df['START_DATETIME_JOB'].dt.year

# Group per month across years
month_counts = df.groupby(['Year', 'Month_Sort', 'Month_Label']).size().reset_index(name='Jumlah')
month_counts = month_counts.sort_values(by=['Month_Sort'])

# Build x-axis (sorted unique months)
all_months   = sorted(month_counts['Month_Sort'].unique())
month_labels = month_counts.drop_duplicates('Month_Sort').set_index('Month_Sort').loc[all_months]['Month_Label'].tolist()

plt.figure(figsize=(14,7))
# Bar for total jobs per month (with all years combined)
total_per_month = month_counts.groupby('Month_Sort')['Jumlah'].sum().reindex(all_months).fillna(0)
plt.bar(range(len(all_months)), total_per_month, color='green', label='Jobs Executed')

# Yearly average lines (dotted)
years  = month_counts['Year'].unique()
colors = plt.cm.get_cmap('tab10', len(years))
for i, year in enumerate(years):
    data_year = month_counts[month_counts['Year'] == year]
    avg_jumlah = data_year['Jumlah'].mean()
    plt.axhline(y=avg_jumlah, color=colors(i), linestyle='--', linewidth=2,
                label=f'Avg {year}: {avg_jumlah:.1f} jobs/month')

plt.title('BASELINE — Actual Execution Rate per Month (All Years)')
plt.xlabel('Execution Date')
plt.ylabel('Jobs Executed')
plt.xticks(ticks=range(len(all_months)), labels=month_labels, rotation=45)
plt.legend()
plt.tight_layout()
plt.show()

# ==== SCATTER: Production over time (actual) ====
# Convert START_DATE_JOB before plotting (if not yet)
df['START_DATE_JOB'] = pd.to_datetime(df['START_DATE_JOB'])

plt.figure(figsize=(10, 6))
sns.scatterplot(data=df, x='START_DATE_JOB', y='BOPD', hue='WELL_ALIAS', palette='Set1', s=100)
plt.title('Scatter of Executed Jobs vs. Production Output Over Time')
plt.xlabel('Execution Date')
plt.ylabel('BOPD')
plt.xticks(rotation=45)
plt.legend().remove()
plt.grid(True)
plt.show()

# ==== VALID DATA SUBSET ====
df_valid = df[(df['BOPD'] > 0) &
              df['SURFACE_LATITUDE'].notnull() &
              df['SURFACE_LONGITUDE'].notnull()].copy()
df_valid = df_valid.reset_index(drop=True)

# Dynamic quantiles for BOPD (for visualization)
q1 = df_valid['BOPD'].quantile(0.25)
q2 = df_valid['BOPD'].quantile(0.50)
q3 = df_valid['BOPD'].quantile(0.75)
print(f"Q1 (25%): {q1:.2f}, Q2 (Median): {q2:.2f}, Q3 (75%): {q3:.2f}")

# Histogram with tier thresholds (visual only)
plt.figure(figsize=(10, 6))
plt.hist(df_valid['BOPD'], bins=15, color='skyblue', edgecolor='black')
plt.axvline(q1, color='blue',  linestyle='--', label=f'Tier-3 (Q1) (≤ {q1:.1f})')
plt.axvline(q2, color='green', linestyle='--', label=f'Tier-2 (Median) ({q2:.1f})')
plt.axvline(q3, color='red',   linestyle='--', label=f'Tier-1 (Q3) (≥ {q3:.1f})')
plt.title("Histogram of BOPD Distribution with Tier Thresholds")
plt.xlabel("BOPD")
plt.ylabel("Number of Wells")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# ==== DISTANCE (Haversine) & MATRIX ====
def haversine(lat1, lon1, lat2, lon2):
    """Great-circle distance (Haversine) in km."""
    R = 6371.0  # Earth radius in kilometers
    phi1, phi2 = np.radians(lat1), np.radians(lat2)
    dphi       = np.radians(lat2 - lat1)
    dlmb       = np.radians(lon2 - lon1)
    a = np.sin(dphi/2.0)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(dlmb/2.0)**2
    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))
    return R * c

def create_distance_matrix(locations):
    """Pairwise distance matrix (km) for a list of (lat, lon)."""
    n = len(locations)
    matrix = np.zeros((n, n))
    for i in range(n):
        lat1, lon1 = locations[i]
        for j in range(n):
            if i != j:
                lat2, lon2 = locations[j]
                matrix[i][j] = haversine(lat1, lon1, lat2, lon2)
    return matrix

# Build a reference distance matrix from all available wells (for preview)
df_clean   = df.dropna(subset=['SURFACE_LATITUDE', 'SURFACE_LONGITUDE', 'WELL_ALIAS'])
locations0 = list(zip(df_clean['SURFACE_LATITUDE'], df_clean['SURFACE_LONGITUDE']))
well_ids   = df_clean['WELL_ALIAS'].values
distance_matrix0 = create_distance_matrix(locations0)
df_matrix  = pd.DataFrame(distance_matrix0, index=well_ids, columns=well_ids)
print(df_matrix.round(2).iloc[:6, :6])   # preview 6x6 sub-table

# ==== GA OBJECTIVE & OPERATORS (GA-1) ====
def calculate_objective(route, matrix, bopd_list, duration_days):
    """
    Fitness/objective: sum over route of (travel_time + job_time) / production (BOPD).
    Lower is better.
    """
    total_score = 0
    current_time = 0
    for i in range(len(route)):
        well_idx = route[i]
        if i == 0:
            travel_time = 0
        else:
            travel_time = matrix[route[i - 1]][well_idx] / 20  # 20 km/day speed assumption

        job_time = duration_days[well_idx]
        if job_time < 0:
            job_time = 0
        total_time = travel_time + job_time
        production = bopd_list[well_idx]
        current_time += total_time
        score = total_time / production if production > 0 else float('inf')
        total_score += score
    return total_score

def create_route(n):
    """Create a random route that always starts at index 0."""
    route = list(range(1, n))    # all indices except 0
    random.shuffle(route)
    return [0] + route

# Quick sanity test on route builder (kept for parity)
n = 10
route = create_route(n)
print(route)
print("Starts at 0?", route[0] == 0)
print("All wells unique?", len(set(route)) == n)

def crossover(p1, p2):
    """Ordered crossover preserving index 0 at the start."""
    size = len(p1)
    start, end = sorted(random.sample(range(1, size), 2))  # cut points start from 1
    child = [-1] * size
    child[start:end] = p1[start:end]
    fill = [item for item in p2 if item not in child and item != 0]
    pointer = 0
    for i in range(1, size):
        if child[i] == -1:
            child[i] = fill[pointer]
            pointer += 1
    child[0] = 0
    return child

def mutate(route, rate=0.02):
    """Swap mutation excluding index 0 (kept as start)."""
    for i in range(1, len(route)):
        if random.random() < rate:
            j = random.randint(1, len(route) - 1)
            route[i], route[j] = route[j], route[i]
    return route

def genetic_algorithm(matrix, bopd_list, duration_days, generations=500, pop_size=400):
    """Simple GA loop: elitism + tournament-like parent sampling."""
    best_scores = []
    population = [create_route(len(matrix)) for _ in range(pop_size)]
    for gen in range(generations):
        population.sort(key=lambda x: calculate_objective(x, matrix, bopd_list, duration_days))
        best_scores.append(calculate_objective(population[0], matrix, bopd_list, duration_days))
        next_gen = population[:5]  # elitism: top 5
        while len(next_gen) < pop_size:
            parents = random.sample(population[:20], 2)  # select from top-20
            child = mutate(crossover(parents[0], parents[1]))
            next_gen.append(child)
        population = next_gen
    best_route = min(population, key=lambda x: calculate_objective(x, matrix, bopd_list, duration_days))
    best_score = calculate_objective(best_route, matrix, bopd_list, duration_days)
    return best_route, best_score, best_scores

# ==== GA INPUTS ====
# Valid wells (positive BOPD + valid coordinates)
df_valid = df[(df['BOPD'] > 0) & df['SURFACE_LATITUDE'].notnull() & df['SURFACE_LONGITUDE'].notnull()].copy()

# Slight jitter for duplicate coordinates (avoid zero-distance artifacts)
duplicate_mask = df_valid.duplicated(subset=['SURFACE_LATITUDE', 'SURFACE_LONGITUDE'], keep=False)
df_valid.loc[duplicate_mask, 'SURFACE_LATITUDE']  += np.random.uniform(-0.0001, 0.0001, size=duplicate_mask.sum())
df_valid.loc[duplicate_mask, 'SURFACE_LONGITUDE'] += np.random.uniform(-0.0001, 0.0001, size=duplicate_mask.sum())

# Compute duration in days for GA (subtract moving time if present)
df_valid['duration_days'] = (
    (df_valid['END_DATETIME_JOB'] - df_valid['START_DATETIME_JOB'] - pd.to_timedelta(df_valid['MOVING_TIME'], unit='D'))
    .dt.total_seconds() / 86400
).fillna(2)  # default 2 days if missing

bopd_list    = df_valid['BOPD'].tolist()
duration_days = df_valid['duration_days'].tolist()
locations     = list(zip(df_valid['SURFACE_LATITUDE'], df_valid['SURFACE_LONGITUDE']))
duration_days = df_valid['duration_days'].tolist()  # keep as original

distance_matrix = create_distance_matrix(locations)

# ==== RUN GA ====
best_route, best_score, best_scores = genetic_algorithm(distance_matrix, bopd_list, duration_days)

print(f'Best route score: {best_score:.2f}')
print("Best Route (Index):", best_route)
print("Well Order:", df_valid.iloc[best_route]['WELL_ALIAS'].tolist())
total_bopd = sum([bopd_list[i] for i in best_route])
print(f'Total BOPD from best route: {total_bopd:.1f}')

# ==== CONVERGENCE PLOT ====
plt.figure(figsize=(10,5))
plt.plot(best_scores, marker='o')
plt.title('Convergence (Approach GA-1)')
plt.xlabel('Generation')
plt.ylabel('Best Fitness Score')
plt.grid(True)
plt.text(0, best_scores[0], f'{best_scores[0]:.2f}', ha='left', va='bottom', fontsize=9, color='green')
plt.text(len(best_scores)-1, best_scores[-1], f'{best_scores[-1]:.2f}', ha='right', va='bottom', fontsize=9, color='red')
plt.tight_layout()
plt.show()

# ==== ROUTE EVALUATION & TIMING SIMULATION ====
# Base time from earliest actual start in valid subset
t0 = pd.to_datetime(df_valid['START_DATETIME_JOB'].min())

# Reorder df according to best_route
df_ga = df_valid.iloc[best_route].reset_index(drop=True)

# Travel time between consecutive wells (20 km/day speed)
speed_km_per_day = 20
travel_times = [0]  # first well has no inbound travel
for i in range(1, len(best_route)):
    prev = best_route[i - 1]
    curr = best_route[i]
    distance = distance_matrix[prev][curr]
    travel_time = distance / speed_km_per_day
    travel_times.append(travel_time)

# Accumulate times and simulate dates
df_ga['travel_time'] = travel_times
df_ga['total_time']  = df_ga['duration_days'] + df_ga['travel_time']
df_ga['cum_time']    = df_ga['total_time'].cumsum()
df_ga['executed_date']  = t0 + pd.to_timedelta(df_ga['cum_time'], unit='D')
df_ga['executed_month'] = df_ga['executed_date'].dt.to_period('M')
df_ga['executed_year']  = df_ga['executed_date'].dt.year

# Route summary (English)
print("GA route order:")
print(" → ".join(df_ga['WELL_ALIAS'].tolist()))
print("Travel time max:", df_ga['travel_time'].max(), "days")
print("Total execution time:", df_ga['cum_time'].iloc[-1], "days")

# ==== BASELINE vs GA (Monthly, per year) ====
# Baseline helper columns
df['Month_Label'] = df['START_DATETIME_JOB'].dt.strftime('%B %Y')
df['Month_Sort']  = df['START_DATETIME_JOB'].dt.to_period('M').astype(str)

# GA helper for monthly aggregation
df_ga['Month_Label'] = df_ga['executed_date'].dt.strftime('%B %Y')
df_ga['Month_Sort']  = df_ga['executed_date'].dt.to_period('M').astype(str)

# Loop over union of years
years = sorted(set(df['START_DATETIME_JOB'].dt.year.unique()) | set(df_ga['executed_date'].dt.year.unique()))
for year in years:
    # Filter by year
    baseline = df[df['START_DATETIME_JOB'].dt.year == year].copy()
    ga_year  = df_ga[df_ga['executed_date'].dt.year == year].copy()

    # Month labels
    baseline['Month_Label'] = baseline['START_DATETIME_JOB'].dt.strftime('%B')
    baseline['Month_Sort']  = baseline['START_DATETIME_JOB'].dt.to_period('M').astype(str)
    ga_year['Month_Label']  = ga_year['executed_date'].dt.strftime('%B')
    ga_year['Month_Sort']   = ga_year['executed_date'].dt.to_period('M').astype(str)

    # === JOB EXECUTION COMPARISON (bar) ===
    baseline_jobs = baseline.groupby(['Month_Sort', 'Month_Label']).size().reset_index(name='Baseline Jobs')
    ga_jobs       = ga_year.groupby(['Month_Sort', 'Month_Label']).size().reset_index(name='GA Jobs')
    jobs_compare  = pd.merge(baseline_jobs, ga_jobs, on=['Month_Sort', 'Month_Label'],
                             how='outer').fillna(0).sort_values('Month_Sort')

    avg_jobs_baseline = jobs_compare['Baseline Jobs'].mean()
    avg_jobs_ga       = jobs_compare['GA Jobs'].mean()

    bar_width = 0.4
    x = np.arange(len(jobs_compare))
    plt.figure(figsize=(14,6))
    total_jobs_baseline = jobs_compare['Baseline Jobs'].sum()
    total_jobs_ga       = jobs_compare['GA Jobs'].sum()

    plt.bar(x - bar_width/2, jobs_compare['Baseline Jobs'], width=bar_width, color='steelblue',
            label=f'Baseline ({int(total_jobs_baseline)} jobs)')
    plt.bar(x + bar_width/2, jobs_compare['GA Jobs'],       width=bar_width, color='purple',
            label=f'Genetic Algorithm ({int(total_jobs_ga)} jobs)')

    # Yearly average lines
    plt.axhline(y=avg_jobs_baseline, color='blue',        linestyle='--', linewidth=2, label=f'Avg Baseline : {avg_jobs_baseline:.1f}')
    plt.axhline(y=avg_jobs_ga,       color='darkmagenta', linestyle='--', linewidth=2, label=f'Avg GA : {avg_jobs_ga:.1f}')

    plt.title('Execution Rate per Month: Baseline vs Genetic Algorithm (GA-1)')
    plt.xlabel('Execution Date')
    plt.ylabel('Total Jobs Executed')
    plt.xticks(x, jobs_compare['Month_Label'], rotation=45)
    plt.legend()
    plt.tight_layout()
    plt.show()

    # === PRODUCTION COMPARISON (line) ===
    baseline_prod = baseline.groupby(['Month_Sort', 'Month_Label'])['BOPD'].sum().reset_index(name='Baseline Production')
    ga_prod       = ga_year.groupby(['Month_Sort', 'Month_Label'])['BOPD'].sum().reset_index(name='GA Production')
    prod_compare  = pd.merge(baseline_prod, ga_prod, on=['Month_Sort', 'Month_Label'],
                             how='outer').fillna(0).sort_values('Month_Sort')

    avg_prod_baseline = prod_compare['Baseline Production'].mean()
    avg_prod_ga       = prod_compare['GA Production'].mean()

    plt.figure(figsize=(14,6))
    total_prod_baseline = prod_compare['Baseline Production'].sum()
    total_prod_ga       = prod_compare['GA Production'].sum()

    plt.plot(prod_compare['Month_Label'], prod_compare['Baseline Production'], marker='o', color='steelblue',
             label=f'Baseline ({int(total_prod_baseline)} BOPD)')
    plt.plot(prod_compare['Month_Label'], prod_compare['GA Production'],       marker='o', color='purple',
             label=f'Genetic Algorithm ({int(total_prod_ga)} BOPD)')

    # Average lines
    plt.axhline(y=avg_prod_baseline, color='blue',        linestyle='--', linewidth=2, label=f'Avg Baseline Prod : {avg_prod_baseline:.1f}')
    plt.axhline(y=avg_prod_ga,       color='darkmagenta', linestyle='--', linewidth=2, label=f'Avg GA Prod : {avg_prod_ga:.1f}')

    plt.title('Total Oil Recovery per Month: Baseline vs Genetic Algorithm (GA-1)')
    plt.xlabel('Execution Date')
    plt.ylabel('Total Oil Recovery (BOPD)')
    plt.grid(True)
    plt.xticks(rotation=45)
    plt.legend()
    plt.tight_layout()
    plt.show()
